---
title: I Tried to Teach an AI to Follow Rules. It Tried to Be Too Smart.
description: A real story about why artificial intelligence fails in regulated environments â€” and how changing the role of AI changes everything.
date: "2024-01-02"
authors:
  - repo
image: /images/blog/blog-post-3.png
---

## It all started with a simple question

> *â€œCan we use artificial intelligence to classify regulatory events?â€*

It sounded obvious.  
AI already writes texts, interprets complex documents, answers difficult questionsâ€¦  
why wouldnâ€™t it help classify an environmental or safety incident?

In practice, the answer was: **not the way I initially imagined**.

---

## The first attempt: â€œlet the AI decideâ€

Like many people do today, I started with the most straightforward approach:

- send the event description,
- attach the applicable regulation,
- ask the AI to tell me the correct classification.

Sometimes it worked beautifully.  
Other times, it produced answers that *sounded* intelligent â€” but were simply wrong.

And worse:  
they were wrong **with confidence**.

---

## When â€œsounds rightâ€ is not enough

In one test, the event description said something like:

> *â€œThere was a diesel oil spill that reached a water body.â€*

For anyone working in the field, this is clearly an environmental accident.

The AI responded with something like:

> *â€œThis is an operational incident with potential environmental impact.â€*

Technically elegant.  
Regulatorily incorrect.

The regulation does not ask whether something *seems* like an incident.  
It asks:

- was there a spill?
- did it reach the environment?
- what was the volume?

There is no interpretation.  
There is **a rule**.

That was the moment everything clicked.

---

## The problem wasnâ€™t the AI. It was the role I gave it.

The AI was doing exactly what it was trained to do:

- interpret text,
- infer meaning,
- choose the most likely answer.

The problem is that **regulation does not work on probability**.

It works like a rigid checklist:

> if X happened, then Y applies.

It doesnâ€™t matter if it feels excessive.  
It doesnâ€™t matter if it was small.  
It doesnâ€™t matter if â€œmost casesâ€ would be different.

---

## The analogy that changed everything

Imagine a GPS that, instead of following traffic rules, decided to â€œinterpret the contextâ€:

> â€œThis red light seems optional.â€  
> â€œPeople usually donâ€™t stop here.â€  
> â€œThe risk of an accident is low.â€

That would be a disaster.

And that was exactly how I was trying to use AI â€”  
as an opinionated GPS in an environment that requires **absolute traffic lights**.

---

## The turning point: what if the AI decided nothing?

Instead of asking:

> *â€œWhat is the correct classification?â€*

I started asking:

> *â€œIs this condition met?â€*

The regulation became a **decision tree**, not a block of text.

Questions like:

- was there environmental impact?
- was the product hazardous?
- did the volume exceed a threshold?

The AI does not choose the path.  
It only answers each question **one at a time**.

Like a technical assistant reading the report out loud,  
while the rule itself decides where to go next.

---

## The result was almost anticlimactic â€” and that was a good thing

Suddenly:

- errors became predictable,
- decisions became explainable,
- and when something went wrongâ€¦ it was clear *why*.

The AI stopped being a mysterious oracle  
and became a highly disciplined operator.

It doesnâ€™t *guess*.  
It **checks**.

---

## What I learned from this

The biggest trap when using AI in critical decisions is wanting it to be too intelligent.

In regulated environments, intelligence is not creativity.  
It is **consistency**.

AI works best when it:

- interprets messy human language,
- extracts evidence,
- and respects clear boundaries defined by humans.

---

## The conclusion that stuck

After all this, I ended up with a sentence that now guides the entire project:

> **AI should not decide the rule.  
It should help apply it.**

When we do that, AI stops being a risk â€”  
and becomes a powerful ally.

---

ðŸ‘‰ **In the next article**, the story gets even more interesting:  
the day I realized that **not every ambiguity is an AI failure** â€”  
sometimes, reality itself is ambiguous.
